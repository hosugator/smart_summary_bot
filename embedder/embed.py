# # embedder/embed.py
# embed.py - M4 Mac ìµœì í™” ì„ë² ë”© ëª¨ë“ˆ (TensorFlow ì—†ìŒ)

import pandas as pd
import numpy as np
import pickle
import logging
from datetime import datetime
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA

logger = logging.getLogger(__name__)


class M4SummaryEmbedder:
    """M4 Mac ìµœì í™” ì„ë² ë” (TensorFlow ì—†ìŒ)"""
    
    def __init__(self, embedding_dim=512):
        self.embedding_dim = embedding_dim
        self.vectorizer = None
        self.pca = None
        print(f"ğŸ M4 Mac ìµœì í™” ì„ë² ë” ì´ˆê¸°í™” (ì°¨ì›: {embedding_dim})")
    
    def load_csv(self, csv_path):
        """CSV íŒŒì¼ ë¡œë“œ"""
        df = pd.read_csv(csv_path)
        logger.info(f"CSV ë¡œë“œ: {df.shape[0]}í–‰, {df.shape[1]}ì—´")
        print(f"ğŸ“„ CSV ë¡œë“œ: {df.shape[0]}í–‰, {df.shape[1]}ì—´")
        return df
    
    def extract_summaries(self, df, summary_column='summary'):
        """DataFrameì—ì„œ summary ì»¬ëŸ¼ ì¶”ì¶œ"""
        summaries = df[summary_column].fillna('').astype(str)
        summaries = summaries[summaries.str.strip() != '']  # ë¹ˆ ë¬¸ìì—´ ì œê±°
        
        logger.info(f"ìœ íš¨í•œ ìš”ì•½ í…ìŠ¤íŠ¸: {len(summaries)}ê°œ")
        print(f"ğŸ“ ìœ íš¨í•œ ìš”ì•½ í…ìŠ¤íŠ¸: {len(summaries)}ê°œ")
        return summaries.tolist()
    
    def create_embeddings(self, texts, batch_size=32):
        """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜"""
        print(f"ğŸ“Š {len(texts)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹œì‘...")
        
        # TF-IDF ë²¡í„°í™” (í•œêµ­ì–´ ìµœì í™”)
        self.vectorizer = TfidfVectorizer(
            max_features=3000,      # íŠ¹ì„± ìˆ˜ ì¦ê°€
            ngram_range=(1, 3),     # 3-gramê¹Œì§€ ì‚¬ìš©
            min_df=1,               # ìµœì†Œ ë¹ˆë„ ë‚®ì¶¤
            max_df=0.9,             # ìµœëŒ€ ë¹ˆë„ ë†’ì„
            analyzer='word',
            token_pattern=r'\b\w+\b'
        )
        
        try:
            tfidf_matrix = self.vectorizer.fit_transform(texts)
            print(f"   TF-IDF ë§¤íŠ¸ë¦­ìŠ¤: {tfidf_matrix.shape}")
            logger.info(f"TF-IDF ë§¤íŠ¸ë¦­ìŠ¤: {tfidf_matrix.shape}")
        except Exception as e:
            print(f"   TF-IDF ì˜¤ë¥˜: {e}")
            logger.error(f"TF-IDF ì˜¤ë¥˜: {e}")
            return None
        
        # PCA ì°¨ì› ì¶•ì†Œ
        n_components = min(self.embedding_dim, tfidf_matrix.shape[1], len(texts))
        print(f"   PCA ì°¨ì›: {tfidf_matrix.shape[1]} â†’ {n_components}")
        
        self.pca = PCA(n_components=n_components, random_state=42)
        
        try:
            embeddings = self.pca.fit_transform(tfidf_matrix.toarray())
            
            # ì°¨ì› íŒ¨ë”© (512ì°¨ì› ë§ì¶”ê¸°)
            if embeddings.shape[1] < self.embedding_dim:
                padding = np.zeros((embeddings.shape[0], self.embedding_dim - embeddings.shape[1]))
                embeddings = np.hstack([embeddings, padding])
                print(f"   íŒ¨ë”© ì ìš©: {embeddings.shape}")
            
            print(f"âœ… ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
            print(f"   ë²”ìœ„: {embeddings.min():.3f} ~ {embeddings.max():.3f}")
            logger.info(f"ì„ë² ë”© ì™„ë£Œ: {embeddings.shape}")
            
            return embeddings
            
        except Exception as e:
            print(f"   PCA ì˜¤ë¥˜: {e}")
            logger.error(f"PCA ì˜¤ë¥˜: {e}")
            return None
    
    def save_embeddings(self, embeddings, texts, original_df, output_path):
        """ì„ë² ë”© ê²°ê³¼ ì €ì¥"""
        # 1. NumPy ë°°ì—´ ì €ì¥ (ëª¨ë¸ í•™ìŠµìš©)
        np.save(f"{output_path}_embeddings.npy", embeddings)
        
        # 2. ì „ì²´ ë°ì´í„° Pickle ì €ì¥
        data = {
            'embeddings': embeddings,
            'texts': texts,
            'original_data': original_df,
            'embedding_dim': embeddings.shape[1],
            'timestamp': datetime.now().isoformat()
        }
        
        with open(f"{output_path}_data.pkl", 'wb') as f:
            pickle.dump(data, f)
        
        logger.info(f"ì €ì¥ ì™„ë£Œ: {output_path}_embeddings.npy, {output_path}_data.pkl")
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {output_path}_embeddings.npy, {output_path}_data.pkl")
    
    def fit_transform(self, texts):
        """ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤: í…ìŠ¤íŠ¸ â†’ ì„ë² ë”©"""
        return self.create_embeddings(texts)
    
    def process_csv_to_embeddings(self, csv_path, summary_column='summary', output_path=None):
        """CSV â†’ ì„ë² ë”© ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
        try:
            # 1. ë°ì´í„° ë¡œë“œ
            df = self.load_csv(csv_path)
            summaries = self.extract_summaries(df, summary_column)
            
            # 2. ì„ë² ë”© ìƒì„±
            embeddings = self.create_embeddings(summaries)
            
            if embeddings is None:
                return {'success': False, 'error': 'ì„ë² ë”© ìƒì„± ì‹¤íŒ¨'}
            
            # 3. ê²°ê³¼ ì €ì¥
            if output_path is None:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                output_path = f"m4_embeddings_{timestamp}"
            
            self.save_embeddings(embeddings, summaries, df, output_path)
            
            return {
                'success': True,
                'total_texts': len(summaries),
                'embedding_shape': embeddings.shape,
                'output_path': output_path
            }
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}


def create_dummy_csv(filename="test_summary.csv", num_rows=100):
    """í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„° ìƒì„±"""
    print("ğŸ”§ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì¤‘...")
    
    # ë‹¤ì–‘í•œ ì£¼ì œì˜ ë”ë¯¸ ë°ì´í„°
    topics = [
        "AI ì¸ê³µì§€ëŠ¥ ê¸°ìˆ  ë°œì „ ë¨¸ì‹ ëŸ¬ë‹ ë”¥ëŸ¬ë‹ ìì—°ì–´ì²˜ë¦¬",
        "ê²½ì œ ê¸ˆìœµ íˆ¬ì ì£¼ì‹ ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ ë¶„ì„",
        "ì •ì¹˜ ì„ ê±° ì •ì±… êµ­ì • ì™¸êµ êµ­ì œê´€ê³„ ì •ë¶€",
        "ì‚¬íšŒ ë¬¸í™” êµìœ¡ ë³µì§€ í™˜ê²½ ê¸°í›„ë³€í™” ì§€ì†ê°€ëŠ¥",
        "ê³¼í•™ ê¸°ìˆ  ì—°êµ¬ ê°œë°œ í˜ì‹  ë°œê²¬ ì‹¤í—˜",
        "ìŠ¤í¬ì¸  ì¶•êµ¬ ì•¼êµ¬ ë†êµ¬ ì˜¬ë¦¼í”½ ê²½ê¸° ì„ ìˆ˜",
        "ì—”í„°í…Œì¸ë¨¼íŠ¸ ì˜í™” ë“œë¼ë§ˆ ìŒì•… ì—°ì˜ˆ ë¬¸í™”",
        "ê±´ê°• ì˜ë£Œ ì§ˆë³‘ ì¹˜ë£Œ ì˜ˆë°© ë°±ì‹  ë³‘ì›",
        "ì—¬í–‰ ê´€ê´‘ ë¬¸í™” ìŒì‹ ì¶•ì œ ì§€ì—­ ëª…ì†Œ",
        "IT ìŠ¤ë§ˆíŠ¸í° ì»´í“¨í„° ì†Œí”„íŠ¸ì›¨ì–´ ì•± ë””ì§€í„¸"
    ]
    
    dummy_data = {
        'url': [f'https://news-example.com/article_{i}' for i in range(num_rows)],
        'title': [f'{topics[i % len(topics)].split()[0]} ê´€ë ¨ ë‰´ìŠ¤ {i}' for i in range(num_rows)],
        'summary': [
            f'{topics[i % len(topics)]}ì— ëŒ€í•œ ìš”ì•½ì…ë‹ˆë‹¤. '
            f'ìµœê·¼ ë™í–¥ê³¼ ì „ë§ì„ ë¶„ì„í•œ ê¸°ì‚¬ {i}ë²ˆì…ë‹ˆë‹¤. '
            f'ì „ë¬¸ê°€ë“¤ì€ í–¥í›„ ë°œì „ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.'
            for i in range(num_rows)
        ]
    }
    
    df = pd.DataFrame(dummy_data)
    df.to_csv(filename, index=False, encoding='utf-8')
    
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ: {filename} ({num_rows}í–‰, {len(topics)}ê°œ ì£¼ì œ)")
    return filename


# ì‚¬ìš© ì˜ˆì‹œ ë° í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    print("ğŸš€ M4 ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹œì‘!")
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
    csv_file = create_dummy_csv("m4_test.csv", 50)
    
    # ì„ë² ë” ì‚¬ìš©
    embedder = M4SummaryEmbedder(embedding_dim=512)
    
    result = embedder.process_csv_to_embeddings(
        csv_path=csv_file,
        summary_column="summary",
        output_path="m4_test_embeddings"
    )
    
    print("\nğŸ“Š ê²°ê³¼:", result)
    
    if result['success']:
        print("ğŸ‰ M4 ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
    else:
        print("âŒ M4 ì„ë² ë”© ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨!")
        
# import pandas as pd
# import numpy as np
# import tensorflow as tf
# import tensorflow_hub as hub
# import pickle
# import logging
# from datetime import datetime

# logger = logging.getLogger(__name__)


# class SummaryEmbedder:
#     """CSV summary ì»¬ëŸ¼ì„ ì„ë² ë”©í•˜ëŠ” í´ë˜ìŠ¤"""
    
#     def __init__(self):
#         """ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”"""
#         self.model_url = "https://tfhub.dev/google/universal-sentence-encoder-multilingual/3"
#         self.embed_model = None
#         self._load_model()
    
#     def _load_model(self):
#         """TensorFlow Hub ëª¨ë¸ ë¡œë“œ"""
#         logger.info("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
#         self.embed_model = hub.load(self.model_url)
#         logger.info("ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
#     def load_csv(self, csv_path):
#         """CSV íŒŒì¼ ë¡œë“œ"""
#         df = pd.read_csv(csv_path)
#         logger.info(f"CSV ë¡œë“œ: {df.shape[0]}í–‰, {df.shape[1]}ì—´")
#         return df
    
#     def extract_summaries(self, df, summary_column='summary'):
#         """DataFrameì—ì„œ summary ì»¬ëŸ¼ ì¶”ì¶œ"""
#         summaries = df[summary_column].fillna('').astype(str)
#         summaries = summaries[summaries.str.strip() != '']  # ë¹ˆ ë¬¸ìì—´ ì œê±°
        
#         logger.info(f"ìœ íš¨í•œ ìš”ì•½ í…ìŠ¤íŠ¸: {len(summaries)}ê°œ")
#         return summaries.tolist()
    
#     def create_embeddings(self, texts, batch_size=32):
#         """í…ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜"""
#         embeddings = []
#         total_batches = (len(texts) + batch_size - 1) // batch_size
        
#         for i in range(0, len(texts), batch_size):
#             batch = texts[i:i + batch_size]
#             batch_num = (i // batch_size) + 1
            
#             logger.info(f"ì„ë² ë”© ìƒì„± ì¤‘: {batch_num}/{total_batches}")
            
#             try:
#                 batch_embeddings = self.embed_model(batch)
#                 embeddings.extend(batch_embeddings.numpy())
#             except Exception as e:
#                 logger.error(f"ë°°ì¹˜ {batch_num} ì‹¤íŒ¨: {e}")
#                 embeddings.extend([np.zeros(512) for _ in batch])
        
#         embeddings_array = np.array(embeddings)
#         logger.info(f"ì„ë² ë”© ì™„ë£Œ: {embeddings_array.shape}")
#         return embeddings_array
    
#     def save_embeddings(self, embeddings, texts, original_df, output_path):
#         """ì„ë² ë”© ê²°ê³¼ ì €ì¥"""
#         # 1. NumPy ë°°ì—´ ì €ì¥ (ëª¨ë¸ í•™ìŠµìš©)
#         # npy ì‚¬ìš© ì´ìœ  : í•œ ë²ˆ ì„ë² ë”© í•˜ë©´ ê²Œì† ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë©°, í…ìŠ¤íŠ¸ ì¬ì²˜ë¦¬ ì—†ì´ ë°”ë¡œ ìˆ«ì ë¡œë“œí•˜ì—¬ ì†ë„ì ì¸ ì¸¡ë©´ì—ì„œ íš¨ìœ¨ì ì„.
#         np.save(f"{output_path}_embeddings.npy", embeddings)
        
#         # 2. ì „ì²´ ë°ì´í„° Pickle ì €ì¥
#         data = {
#             'embeddings': embeddings,
#             'texts': texts,
#             'original_data': original_df,
#             'embedding_dim': embeddings.shape[1],
#             'timestamp': datetime.now().isoformat()
#         }
        
#         with open(f"{output_path}_data.pkl", 'wb') as f:
#             pickle.dump(data, f)
        
#         logger.info(f"ì €ì¥ ì™„ë£Œ: {output_path}_embeddings.npy, {output_path}_data.pkl")
    
#     def process_csv_to_embeddings(self, csv_path, summary_column='summary', output_path=None):
#         """CSV â†’ ì„ë² ë”© ì „ì²´ í”„ë¡œì„¸ìŠ¤"""
#         try:
#             # 1. ë°ì´í„° ë¡œë“œ
#             df = self.load_csv(csv_path)
#             summaries = self.extract_summaries(df, summary_column)
            
#             # 2. ì„ë² ë”© ìƒì„±
#             embeddings = self.create_embeddings(summaries)
            
#             # 3. ê²°ê³¼ ì €ì¥
#             if output_path is None:
#                 timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#                 output_path = f"embeddings_{timestamp}"
            
#             self.save_embeddings(embeddings, summaries, df, output_path)
            
#             return {
#                 'success': True,
#                 'total_texts': len(summaries),
#                 'embedding_shape': embeddings.shape,
#                 'output_path': output_path
#             }
            
#         except Exception as e:
#             logger.error(f"ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
#             return {'success': False, 'error': str(e)}


# # ì‚¬ìš© ì˜ˆì‹œ
# if __name__ == "__main__":
#     logging.basicConfig(level=logging.INFO)
    
#     embedder = SummaryEmbedder()
#     result = embedder.process_csv_to_embeddings(
#         csv_path="result_summary.csv",
#         summary_column="summary",
#         output_path="training_embeddings"
#     )
    
#     print("ê²°ê³¼:", result)